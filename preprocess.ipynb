{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc17a8a8",
   "metadata": {},
   "source": [
    "功能概述：\n",
    "构建完整的数据获取与清洗流水线。负责整合游戏卡池流水数据，爬取 B 站指定视频及其评论，并进行深度的文本清洗，为后续模型训练提供标准语料。\n",
    "\n",
    "输入：\n",
    "- 游戏版本记录与外部流水数据 (CSV)\n",
    "- B 站视频搜索规则配置\n",
    "输出：\n",
    "- ./data/character_info.csv (整合后的角色7日流水标签)\n",
    "- ./data/bilibili_video_data_final.csv (爬取的视频元数据)\n",
    "- ./data/all_comments_merged.csv (最终清洗、合并并打标的评论语料)\n",
    "\n",
    "脚本执行流程：\n",
    "\n",
    "1. 【卡池与流水整合】\n",
    "   - 读取版本记录与七麦流水数据，对齐时间轴。\n",
    "   - 剔除异常数据，计算每个角色卡池的“首周（7日）流水”作为预测目标 $y$。\n",
    "\n",
    "2. 【视频元数据爬取】\n",
    "   - 实现了 Bilibili WBI 签名算法以绕过接口验证。\n",
    "   - 根据预设规则（角色PV、走近星穹、头部攻略 UP 主），针对每个角色精准搜索对应的三个特定视频。\n",
    "\n",
    "3. 【视频评论爬取】\n",
    "   - 使用[1dyer的b站视频爬虫](https://github.com/1dyer/bilibili-comment-crawler)爬取评论\n",
    "   - 添加了断点续传机制\n",
    "\n",
    "4. 【文本清洗与去重】\n",
    "   - 文本规范化：处理 B 站特有的 [表情包]、Emoji、颜文字等。\n",
    "   - 繁简转换与符号清洗：统一字符编码，去除无意义符号。\n",
    "   - 对同一 ID 的重复评论，保留点赞数最高的一条。\n",
    "\n",
    "5. 【数据合并】\n",
    "   - 计算每条评论距离视频发布的时间差。\n",
    "   - 仅保留视频发布后 24 小时内的评论（防止泄漏），输出最终用于训练的合并文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66614fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. 加载数据 ---\n",
    "# 加载版本卡池信息CSV\n",
    "versions_df = pd.read_csv('./data/崩坏：星穹铁道版本记录_2025-10-27.csv')\n",
    "revenue_df = pd.read_csv('./data/2023-04-26至2025-10-26 iPhone收入预估.csv')\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. 构造完整的卡池列表 ---\n",
    "# 分别处理第一个和第二个卡池\n",
    "pool1 = versions_df[['版本号', '卡池1', '卡池角色1']].rename(columns={'卡池1': 'start_date', '卡池角色1': 'character'})\n",
    "pool1['pool_index'] = 1\n",
    "pool2 = versions_df[['版本号', '卡池2', '卡池角色2']].rename(columns={'卡池2': 'start_date', '卡池角色2': 'character'})\n",
    "pool2['pool_index'] = 2\n",
    "\n",
    "# 合并成一个DataFrame\n",
    "all_pools_df = pd.concat([pool1, pool2])\n",
    "\n",
    "# --- 3. 数据清洗与预处理 ---\n",
    "# 转换日期格式\n",
    "all_pools_df['start_date'] = pd.to_datetime(all_pools_df['start_date'])\n",
    "\n",
    "# 按开始日期排序\n",
    "all_pools_df.sort_values('start_date', inplace=True, ignore_index=True)\n",
    "\n",
    "# --- 4. 计算结束日期和持续时间 ---\n",
    "# 下一个卡池的开始时间即为当前卡池的结束边界\n",
    "all_pools_df['end_date'] = all_pools_df['start_date'].shift(-1) - pd.Timedelta(days=1)\n",
    "\n",
    "# 直接删除没有有效结束日期的行（也就是最后一个卡池）\n",
    "all_pools_df.dropna(subset=['end_date'], inplace=True)\n",
    "\n",
    "\n",
    "# 筛选掉指定的不分析的角色\n",
    "pools_to_analyze_df = all_pools_df[~all_pools_df['character'].isin(['真理医生', '丹恒·腾荒', '无', '白厄', 'Saber'])].copy()\n",
    "\n",
    "# 计算持续天数\n",
    "pools_to_analyze_df['duration_days'] = (pools_to_analyze_df['end_date'] - pools_to_analyze_df['start_date']).dt.days + 1\n",
    "\n",
    "\n",
    "# --- 5. 整合流水数据 ---\n",
    "# 预处理流水数据\n",
    "revenue_df.rename(columns={'收入预估~iPhone': 'revenue'}, inplace=True)\n",
    "revenue_df['日期'] = pd.to_datetime(revenue_df['日期'])\n",
    "revenue_df.set_index('日期', inplace=True)\n",
    "\n",
    "# 为每个卡池计算总流水和7日流水\n",
    "total_revenues = []\n",
    "seven_day_revenues = [] # <-- 新增一个列表来存储7日流水\n",
    "for index, row in pools_to_analyze_df.iterrows():\n",
    "    # --- 计算总流水 (逻辑不变) ---\n",
    "    total_mask = (revenue_df.index >= row['start_date']) & (revenue_df.index <= row['end_date'])\n",
    "    total_revenue = revenue_df.loc[total_mask, 'revenue'].sum()\n",
    "    total_revenues.append(total_revenue)\n",
    "\n",
    "    # --- 计算7日流水 (新增逻辑) ---\n",
    "    # 定义7日窗口的结束日期 (开始日期 + 6天)\n",
    "    seven_day_end_date = row['start_date'] + pd.Timedelta(days=6)\n",
    "    # 筛选出在7日窗口内的流水数据\n",
    "    seven_day_mask = (revenue_df.index >= row['start_date']) & (revenue_df.index <= seven_day_end_date)\n",
    "    seven_day_revenue = revenue_df.loc[seven_day_mask, 'revenue'].sum()\n",
    "    seven_day_revenues.append(seven_day_revenue)\n",
    "\n",
    "pools_to_analyze_df['total_revenue'] = total_revenues\n",
    "pools_to_analyze_df['7day_revenue'] = seven_day_revenues \n",
    "\n",
    "# --- 6. 标记性别 ---\n",
    "# 这是一个让你快速标注的区域。\n",
    "# 请将下面的角色名列表，根据性别分别填入 female_characters 和 male_characters 列表中。\n",
    "\n",
    "female_characters = [\n",
    "    # 在这里粘贴所有女性角色名, 例如:\n",
    "    \"银狼\", \"卡芙卡\", \"符玄\", \"镜流\", '托帕&账账', \"藿藿\", \"阮•梅\", \n",
    "    \"黑天鹅\", \"花火\", \"黄泉\", \"知更鸟\", \"流萤\", \"翡翠\", \"云璃\", \"灵砂\",\n",
    "    \"忘归人\", \"大黑塔\", \"阿格莱雅\", \"缇宝\", \"遐蝶\", \"风堇\", \n",
    "    \"赛飞儿\", \"海瑟音\", \"刻律德菈\", \"长夜月\", \"飞霄\", \"乱破\", \"Saber\"\n",
    "]\n",
    "\n",
    "male_characters = [\n",
    "    # 在这里粘贴所有男性角色名, 例如:\n",
    "    \"刃\", \"丹恒•饮月\", \"银枝\", \"砂金\", \"波提欧\", \"椒丘\",\n",
    "     \"星期日\", \"万敌\", \"罗刹\", \"那刻夏\", \"白厄\", \"无\"\n",
    "]\n",
    "\n",
    "# 自动构建性别映射字典\n",
    "gender_map = {character: '女' for character in female_characters}\n",
    "gender_map.update({character: '男' for character in male_characters})\n",
    "\n",
    "# 将性别信息映射到DataFrame\n",
    "pools_to_analyze_df['gender'] = pools_to_analyze_df['character'].map(gender_map)\n",
    "\n",
    "# (可选) 检查是否有遗漏的角色未被标记\n",
    "unmapped = pools_to_analyze_df[pools_to_analyze_df['gender'].isnull()]\n",
    "if not unmapped.empty:\n",
    "    print(\"\\n[警告] 以下角色没有被分配性别，请检查上面的列表:\")\n",
    "    print(unmapped['character'].unique())\n",
    "\n",
    "# --- 7. 最终结果展示 ---\n",
    "# 调整列顺序以便查看\n",
    "chara_df = pools_to_analyze_df[[\n",
    "    'character', 'gender', 'start_date', 'end_date', 'duration_days', 'total_revenue','7day_revenue', '版本号', 'pool_index'\n",
    "]].copy()\n",
    "\n",
    "# 格式化日期和流水，使其更易读\n",
    "chara_df['start_date'] = chara_df['start_date'].dt.strftime('%Y-%m-%d')\n",
    "chara_df['end_date'] = chara_df['end_date'].dt.strftime('%Y-%m-%d')\n",
    "chara_df['total_revenue'] = chara_df['total_revenue'].apply(lambda x: int(x))\n",
    "\n",
    "print(\"各角色卡池持续时间与总流水统计:\")\n",
    "print(chara_df.to_string())\n",
    "CHARA_FILENAME='./data/character_info.csv'\n",
    "chara_df.to_csv(CHARA_FILENAME, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from hashlib import md5\n",
    "from functools import reduce\n",
    "from urllib.parse import urlencode\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "MAX_NETWORK_RETRIES = 3 # 底层网络请求失败时的最大重试次数\n",
    "RETRY_DELAY_SECONDS = 2 # 每次重试前的等待时间\n",
    "PRECISE_SEARCH_ATTEMPTS = 3 #最大搜索次数\n",
    "# --- WBI签名算法实现 (这部分代码无需修改) ---\n",
    "MIXIN_KEY_ENC_TABLE = [\n",
    "    46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35, 27, 43, 5, 49,\n",
    "    33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13, 37, 48, 7, 16, 24, 55, 40,\n",
    "    61, 26, 17, 0, 1, 60, 51, 30, 4, 22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11,\n",
    "    36, 20, 34, 44, 52\n",
    "]\n",
    "\n",
    "def get_mixin_key(orig: str) -> str:\n",
    "    return reduce(lambda s, i: s + orig[i], MIXIN_KEY_ENC_TABLE, '')[:32]\n",
    "\n",
    "def wbi_sign_params(params: dict, wbi_keys: dict) -> dict:\n",
    "    img_key = wbi_keys['img_key']\n",
    "    sub_key = wbi_keys['sub_key']\n",
    "    \n",
    "    mixin_key = get_mixin_key(img_key + sub_key)\n",
    "    curr_time = round(time.time())\n",
    "    params['wts'] = curr_time\n",
    "    \n",
    "    params = dict(sorted(params.items()))\n",
    "    params = {\n",
    "        k: ''.join(filter(lambda ch: ch not in \"!*\\'()\", str(v)))\n",
    "        for k, v in params.items()\n",
    "    }\n",
    "    query = urlencode(params)\n",
    "    wbi_sign = md5((query + mixin_key).encode()).hexdigest()\n",
    "    params['w_rid'] = wbi_sign\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_wbi_keys(headers: dict) -> dict:\n",
    "    resp = requests.get('https://api.bilibili.com/x/web-interface/nav', headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    json_content = resp.json()\n",
    "    img_url = json_content['data']['wbi_img']['img_url']\n",
    "    sub_url = json_content['data']['wbi_img']['sub_url']\n",
    "    wbi_keys = {\n",
    "        'img_key': img_url.rsplit('/', 1)[1].split('.')[0],\n",
    "        'sub_key': sub_url.rsplit('/', 1)[1].split('.')[0]\n",
    "    }\n",
    "    return wbi_keys\n",
    "# --- WBI签名算法结束 ---\n",
    "\n",
    "\n",
    "# --- 爬虫核心函数 (已重构) ---\n",
    "def _fetch_page_data(params: dict, headers: dict, wbi_keys: dict):\n",
    "    base_url = \"https://api.bilibili.com/x/web-interface/wbi/search/all/v2\"\n",
    "    for attempt in range(MAX_NETWORK_RETRIES):\n",
    "        try:\n",
    "            signed_params = wbi_sign_params(params.copy(), wbi_keys)\n",
    "            response = requests.get(base_url, params=signed_params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"    -> 网络请求失败 (尝试 {attempt + 1}/{MAX_NETWORK_RETRIES}): {e}\")\n",
    "            if attempt < MAX_NETWORK_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "    return None\n",
    "\n",
    "def _parse_and_find_first_match(data: dict, up_name: str, char_name: str, video_type: str, pattern: str):\n",
    "    \"\"\"解析数据并只返回第一个匹配规则的视频\"\"\"\n",
    "    if not data: return None\n",
    "    video_list = []\n",
    "    if 'data' in data and 'result' in data['data']:\n",
    "        for item in data['data']['result']:\n",
    "            if item.get('result_type') == 'video': video_list = item.get('data', []); break\n",
    "    for video in video_list:\n",
    "        if video.get('author') != up_name: continue\n",
    "        clean_title = video.get('title', '').replace('<em class=\"keyword\">', '').replace('</em>', '')\n",
    "        if re.search(pattern, clean_title):\n",
    "            return {\n",
    "                'character': char_name, 'video_type': video_type, 'bvid': video.get('bvid'), \n",
    "                'title': clean_title, 'author': video.get('author'), 'url': video.get('arcurl'), \n",
    "                'play_count': video.get('play'), 'comment_count': video.get('review'), \n",
    "                'danmaku_count': video.get('danmaku'), 'like_count': video.get('like'), \n",
    "                'favorite_count': video.get('favorites'), 'publish_timestamp': video.get('pubdate'), \n",
    "                'duration': video.get('duration'),\n",
    "            }\n",
    "    return None # 没有找到匹配项\n",
    "\n",
    "def find_video_for_rule(char_name, search_name, up_name, rule, formatted_pattern, headers, wbi_keys):\n",
    "    \"\"\"为单个规则执行一个多阶段、带重试的搜索级联。\"\"\"\n",
    "    \n",
    "    # Stage 1: Ultra-Precise Search with Retries\n",
    "    precise_query = f\"{search_name}+{up_name}+{rule['type']}\"\n",
    "    print(f\"  -> Stage 1: 精确搜索 '{precise_query}' (最多 {PRECISE_SEARCH_ATTEMPTS} 次)\")\n",
    "    for attempt in range(PRECISE_SEARCH_ATTEMPTS):\n",
    "        params = {'keyword': precise_query, 'page': 1, 'page_size': 42}\n",
    "        page_data = _fetch_page_data(params, headers, wbi_keys)\n",
    "        video = _parse_and_find_first_match(page_data, up_name, char_name, rule['type'], formatted_pattern)\n",
    "        if video:\n",
    "            print(f\"    -> 在尝试第 {attempt + 1} 次时成功找到!\")\n",
    "            return video\n",
    "        if attempt < PRECISE_SEARCH_ATTEMPTS - 1:\n",
    "            print(f\"    -> 尝试第 {attempt + 1} 次失败，准备重试...\")\n",
    "            time.sleep(RETRY_DELAY_SECONDS)\n",
    "            \n",
    "    # Stage 2: General Search (Fallback)\n",
    "    general_query = f\"{search_name} {up_name}\"\n",
    "    print(f\"  -> Stage 2: 降级到常规搜索 '{general_query}'\")\n",
    "    params = {'keyword': general_query, 'page': 1, 'page_size': 42}\n",
    "    page_data = _fetch_page_data(params, headers, wbi_keys)\n",
    "    video = _parse_and_find_first_match(page_data, up_name, char_name, rule['type'], formatted_pattern)\n",
    "    if video:\n",
    "        print(\"    -> 成功找到!\")\n",
    "        return video\n",
    "\n",
    "    # Stage 3: Broad Search (Final Fallback)\n",
    "    broad_query = search_name\n",
    "    print(f\"  -> Stage 3: 降级到宽泛搜索 '{broad_query}'\")\n",
    "    params = {'keyword': broad_query, 'page': 1, 'page_size': 42}\n",
    "    page_data = _fetch_page_data(params, headers, wbi_keys)\n",
    "    video = _parse_and_find_first_match(page_data, up_name, char_name, rule['type'], formatted_pattern)\n",
    "    if video:\n",
    "        print(\"    -> 成功找到!\")\n",
    "        return video\n",
    "    return None\n",
    "\n",
    "# ===============================================\n",
    "# ========== 需要你手动修改的区域 ==========\n",
    "# ===============================================\n",
    "MY_COOKIE = \"在此处粘贴你的Cookie字符串\"\n",
    "MY_USER_AGENT = \"在此处粘贴你的User-Agent字符串\"\n",
    "\n",
    "CHARACTER_ALIASES = {\n",
    "    \"丹恒•饮月\": \"饮月\",\n",
    "    \"阮•梅\": \"阮梅\",\n",
    "    \"托帕&账账\": \"托帕\",\n",
    "    # 未来可在此添加更多别名\n",
    "}\n",
    "\n",
    "# 2. 视频查询规则配置 (核心)\n",
    "#    - {CHAR} 将被替换为角色的全名或别名\n",
    "#    - pattern 是一个正则表达式\n",
    "SEARCH_CONFIG = {\n",
    "    \"崩坏星穹铁道\": [\n",
    "        {\n",
    "            \"type\": \"角色PV\",\n",
    "            \"pattern\": r\"《崩坏：星穹铁道》.*?{CHAR}.*?角色PV\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"走近星穹\",\n",
    "            \"pattern\": r\"《崩坏：星穹铁道》.*?走近星穹.*?「{CHAR}\"\n",
    "        }\n",
    "    ],\n",
    "    \"卡特亚\": [\n",
    "        {\n",
    "            \"type\": \"综合\",\n",
    "            \"pattern\": r\"{CHAR}.*?综合\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}\n",
    "characters_to_search = list(chara_df['character'])\n",
    "# ===============================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if \"在此处粘贴你的Cookie字符串\" in MY_COOKIE:\n",
    "        print(\"错误：请先在脚本中填入你自己的 MY_COOKIE 和 MY_USER_AGENT！\"); exit()\n",
    "    \n",
    "    CSV_FILENAME = \"./data/bilibili_video_data_final.csv\"\n",
    "\n",
    "    already_found = set()\n",
    "    \n",
    "    try:\n",
    "        existing_df = pd.read_csv(CSV_FILENAME)\n",
    "        # 使用zip和set comprehension高效创建查询集合\n",
    "        already_found = set(zip(existing_df['character'], existing_df['author'], existing_df['video_type']))\n",
    "        print(f\"成功读取已有的 {len(existing_df)} 条记录 from '{CSV_FILENAME}'\\n\")\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame() # 如果文件不存在，创建一个空的DataFrame\n",
    "        print(f\"未找到 '{CSV_FILENAME}'。将从头开始爬取。\\n\")\n",
    "    # --- 断点续爬逻辑结束 ---\n",
    "\n",
    "    request_headers = { 'User-Agent': MY_USER_AGENT, 'Cookie': MY_COOKIE, 'Referer': 'https://search.bilibili.com/' }\n",
    "    \n",
    "    print(\"正在获取最新的WBI密钥...\"); \n",
    "    try: wbi_keys = get_wbi_keys(request_headers); print(\"WBI密钥获取成功!\\n\")\n",
    "    except Exception as e: print(f\"致命错误：获取WBI密钥失败，程序将退出。错误信息: {e}\"); exit()\n",
    "        \n",
    "    newly_found_videos = [] # 只存储本次新发现的视频\n",
    "\n",
    "    for char_name in characters_to_search:\n",
    "        search_name = CHARACTER_ALIASES.get(char_name, char_name)\n",
    "        alias = CHARACTER_ALIASES.get(char_name)\n",
    "        pattern_char_name = f\"({re.escape(char_name)}|{re.escape(alias)})\" if alias else re.escape(char_name)\n",
    "        \n",
    "        for up_name, rules in SEARCH_CONFIG.items():\n",
    "            for rule in rules:\n",
    "                lookup_key = (char_name, up_name, rule['type'])\n",
    "                if lookup_key in already_found:\n",
    "                    print(f\"--- 已存在记录，跳过 (UP: '{up_name}', Char: '{char_name}', Type: '{rule['type']}') ---\")\n",
    "                    continue # 跳过本次循环，进行下一个\n",
    "                \n",
    "                print(f\"--- 开始为 (UP主: '{up_name}', 角色: '{char_name}', 类型: '{rule['type']}') 查找视频 ---\")\n",
    "                \n",
    "                formatted_pattern = rule['pattern'].format(CHAR=pattern_char_name)\n",
    "                found_video = find_video_for_rule(char_name, search_name, up_name, rule, formatted_pattern, request_headers, wbi_keys)\n",
    "                \n",
    "                if found_video:\n",
    "                    newly_found_videos.append(found_video)\n",
    "                else:\n",
    "                    print(f\"[警告] 所有搜索阶段均失败，未能找到 (UP: '{up_name}', Char: '{char_name}', Type: '{rule['type']}') 的视频。\")\n",
    "                \n",
    "                print(\"-\" * 70)\n",
    "                time.sleep(np.random.uniform(1,3))\n",
    "\n",
    "    if newly_found_videos or not existing_df.empty:\n",
    "        new_df = pd.DataFrame(newly_found_videos)\n",
    "        # 合并旧数据和新数据\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        \n",
    "        # 对合并后的完整数据进行后处理\n",
    "        combined_df.drop_duplicates(subset=['character', 'author', 'video_type'], inplace=True, keep='last')\n",
    "        \n",
    "        # 只有在timestamp列存在时才进行转换\n",
    "        if 'publish_timestamp' in combined_df.columns:\n",
    "            combined_df['publish_date'] = pd.to_datetime(combined_df['publish_timestamp'], unit='s').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        sorted_df = combined_df.sort_values(by=['character', 'author', 'video_type']).reset_index(drop=True)\n",
    "\n",
    "        print(\"\\n=============== 任务完成 ================\")\n",
    "        print(f\"本次运行新增了 {len(newly_found_videos)} 条记录。\")\n",
    "        print(f\"总共 {len(sorted_df)} 条记录已保存到 '{CSV_FILENAME}'\")\n",
    "        \n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_colwidth', 40)\n",
    "        \n",
    "        display_columns = ['character', 'author', 'video_type', 'title', 'play_count', 'comment_count', 'bvid']\n",
    "        print(sorted_df[display_columns])\n",
    "        \n",
    "        sorted_df.to_csv(CSV_FILENAME, index=False, encoding='utf-8-sig')\n",
    "    else:\n",
    "        print(\"\\n任务完成，但未能找到任何符合查询规则的视频。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import urllib\n",
    "import time\n",
    "import csv\n",
    "import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# 通过bv号，获取视频的oid\n",
    "def get_information(bv):\n",
    "    resp = requests.get(f\"https://www.bilibili.com/video/{bv}/?p=14&spm_id_from=pageDriver&vd_source=cd6ee6b033cd2da64359bad72619ca8a\",headers=HEADER)\n",
    "    # 提取视频oid\n",
    "    obj = re.compile(f'\"aid\":(?P<id>.*?),\"bvid\":\"{bv}\"')\n",
    "    oid = obj.search(resp.text).group('id')\n",
    "\n",
    "    # 提取视频的标题\n",
    "    obj = re.compile(r'<title data-vue-meta=\"true\">(?P<title>.*?)</title>')\n",
    "    try:\n",
    "        title = obj.search(resp.text).group('title')\n",
    "    except:\n",
    "        title = \"未识别\"\n",
    "\n",
    "    return oid,title\n",
    "\n",
    "# MD5加密\n",
    "def md5(code):\n",
    "    MD5 = hashlib.md5()\n",
    "    MD5.update(code.encode('utf-8'))\n",
    "    w_rid = MD5.hexdigest()\n",
    "    return w_rid\n",
    "\n",
    "# 轮页爬取\n",
    "def start(bv, oid, pageID, count, csv_writer, is_second):\n",
    "    # 参数\n",
    "    mode = 3   # 为2时爬取的是最新评论，为3时爬取的是热门评论\n",
    "    plat = 1\n",
    "    type = 1  \n",
    "    web_location = 1315875\n",
    "\n",
    "    # 获取当下时间戳\n",
    "    wts = int(time.time())\n",
    "    \n",
    "    # 如果不是第一页\n",
    "    if pageID != '':\n",
    "        pagination_str = '{\"offset\":\"%s\"}' % pageID\n",
    "        code = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str, safe='')}&plat={plat}&type={type}&web_location={web_location}&wts={wts}\" + 'ea1db124af3c7062474693fa704f4ff8'\n",
    "        w_rid = md5(code)\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/wbi/main?oid={oid}&type={type}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}&plat=1&web_location=1315875&w_rid={w_rid}&wts={wts}\"\n",
    "    \n",
    "    # 如果是第一页\n",
    "    else:\n",
    "        pagination_str = '{\"offset\":\"\"}'\n",
    "        code = f\"mode={mode}&oid={oid}&pagination_str={urllib.parse.quote(pagination_str, safe='')}&plat={plat}&seek_rpid=&type={type}&web_location={web_location}&wts={wts}\" + 'ea1db124af3c7062474693fa704f4ff8'\n",
    "        w_rid = md5(code)\n",
    "        url = f\"https://api.bilibili.com/x/v2/reply/wbi/main?oid={oid}&type={type}&mode={mode}&pagination_str={urllib.parse.quote(pagination_str, safe=':')}&plat=1&seek_rpid=&web_location=1315875&w_rid={w_rid}&wts={wts}\"\n",
    "    \n",
    "\n",
    "    comment = requests.get(url=url, headers=HEADER).content.decode('utf-8')\n",
    "    comment = json.loads(comment)\n",
    "    if comment.get('code') != 0:\n",
    "        print(f\"API返回错误！ Code: {comment.get('code')}, Message: {comment.get('message')}\")\n",
    "        print(\"这通常意味着Cookie失效、WBI签名错误或IP被限制。\")\n",
    "        print(comment)\n",
    "\n",
    "\n",
    "    # ==================== 安全地访问数据 ====================\n",
    "    # 经过上面的检查，现在可以比较安全地假设'data'键存在了\n",
    "    # 但为了绝对安全，我们还是用 .get() 方法\n",
    "    replies_list = comment.get('data', {}).get('replies')\n",
    "    if not replies_list:\n",
    "        print(\"当前页没有评论数据或已到达末页。\")\n",
    "\n",
    "    for reply in comment['data']['replies']:\n",
    "        # 评论数量+1\n",
    "        count += 1\n",
    "\n",
    "        if count % 1000 ==0:\n",
    "            time.sleep(20)\n",
    "\n",
    "        # 上级评论ID\n",
    "        parent=reply[\"parent\"]\n",
    "        # 评论ID\n",
    "        rpid = reply[\"rpid\"]\n",
    "        # 用户ID\n",
    "        uid = reply[\"mid\"]\n",
    "        # 用户名\n",
    "        name = reply[\"member\"][\"uname\"]\n",
    "        # 用户等级\n",
    "        level = reply[\"member\"][\"level_info\"][\"current_level\"]\n",
    "        # 性别\n",
    "        sex = reply[\"member\"][\"sex\"]\n",
    "        # 头像\n",
    "        avatar = reply[\"member\"][\"avatar\"]\n",
    "        # 是否是大会员\n",
    "        if reply[\"member\"][\"vip\"][\"vipStatus\"] == 0:\n",
    "            vip = \"否\"\n",
    "        else:\n",
    "            vip = \"是\"\n",
    "        # IP属地\n",
    "        try:\n",
    "            IP = reply[\"reply_control\"]['location'][5:]\n",
    "        except:\n",
    "            IP = \"未知\"\n",
    "        # 内容\n",
    "        context = reply[\"content\"][\"message\"]\n",
    "        # 评论时间\n",
    "        reply_time = pd.to_datetime(reply[\"ctime\"], unit='s')\n",
    "        # 相关回复数\n",
    "        try:\n",
    "            rereply = reply[\"reply_control\"][\"sub_reply_entry_text\"]\n",
    "            rereply = int(re.findall(r'\\d+', rereply)[0])\n",
    "        except:\n",
    "            rereply = 0\n",
    "        # 点赞数\n",
    "        like = reply['like']\n",
    "\n",
    "\n",
    "        # 写入CSV文件\n",
    "        csv_writer.writerow([count, parent, rpid, uid, name, level, sex, context, reply_time, rereply, like, IP, vip])\n",
    "\n",
    "        # 二级评论(如果开启了二级评论爬取，且该评论回复数不为0，则爬取该评论的二级评论)\n",
    "        if is_second and rereply !=0:\n",
    "            for page in range(1,(rereply-1)//10+2):\n",
    "                second_url=f\"https://api.bilibili.com/x/v2/reply/reply?oid={oid}&type=1&root={rpid}&ps=10&pn={page}&web_location=333.788\"\n",
    "                second_comment=requests.get(url=second_url,headers=HEADER).content.decode('utf-8')\n",
    "                second_comment=json.loads(second_comment)\n",
    "                # pprint.pprint(second_comment, width=80)\n",
    "                if second_comment['data']['replies'] is None:\n",
    "                    print('find None')\n",
    "                    # print(second_url)\n",
    "                    continue\n",
    "                for second in second_comment['data']['replies']:\n",
    "                    # 评论数量+1\n",
    "                    count += 1\n",
    "                    # 上级评论ID\n",
    "                    parent=second[\"parent\"]\n",
    "                    # 评论ID\n",
    "                    second_rpid = second[\"rpid\"]\n",
    "                    # 用户ID\n",
    "                    uid = second[\"mid\"]\n",
    "                    # 用户名\n",
    "                    name = second[\"member\"][\"uname\"]\n",
    "                    # 用户等级\n",
    "                    level = second[\"member\"][\"level_info\"][\"current_level\"]\n",
    "                    # 性别\n",
    "                    sex = second[\"member\"][\"sex\"]\n",
    "                    # 是否是大会员\n",
    "                    if second[\"member\"][\"vip\"][\"vipStatus\"] == 0:\n",
    "                        vip = \"否\"\n",
    "                    else:\n",
    "                        vip = \"是\"\n",
    "                    # IP属地\n",
    "                    try:\n",
    "                        IP = second[\"reply_control\"]['location'][5:]\n",
    "                    except:\n",
    "                        IP = \"未知\"\n",
    "                    # 内容\n",
    "                    context = second[\"content\"][\"message\"]\n",
    "                    # 评论时间\n",
    "                    reply_time = pd.to_datetime(second[\"ctime\"], unit='s')\n",
    "                    # 相关回复数\n",
    "                    try:\n",
    "                        rereply = second[\"reply_control\"][\"sub_reply_entry_text\"]\n",
    "                        rereply = re.findall(r'\\d+', rereply)[0]\n",
    "                    except:\n",
    "                        rereply = 0\n",
    "                    # 点赞数\n",
    "                    like = second['like']\n",
    "\n",
    "\n",
    "                    # 写入CSV文件\n",
    "                    csv_writer.writerow([count, parent, second_rpid, uid, name, level, sex, context, reply_time, rereply, like, IP, vip])\n",
    "\n",
    "                time.sleep(np.random.uniform(0.6,1.2))\n",
    "            \n",
    "\n",
    "\n",
    "    # 下一页的pageID\n",
    "    try:\n",
    "        next_pageID = comment['data']['cursor']['pagination_reply']['next_offset']\n",
    "    except:\n",
    "        next_pageID = 0\n",
    "\n",
    "    # 判断是否是最后一页了\n",
    "    if next_pageID == 0:\n",
    "        print(f\"评论爬取完成！总共爬取{count}条。\")\n",
    "        return bv, oid, next_pageID, count, csv_writer,is_second\n",
    "    # 如果不是最后一页，则停0.5s（避免反爬机制）\n",
    "    else:\n",
    "        time.sleep(np.random.uniform(0.6,1.2))\n",
    "        print(f\"当前爬取{count}条。\")\n",
    "        return bv, oid, next_pageID, count, csv_writer,is_second\n",
    "\n",
    "HEADER={\n",
    "        \"Cookie\":MY_COOKIE,\n",
    "        \"User-Agent\": MY_USER_AGENT\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    CSV_FILENAME = \"./data/bilibili_video_data_final.csv\"\n",
    "    video_df = pd.read_csv(CSV_FILENAME)\n",
    "    for i in range(len(video_df)):\n",
    "        bv = video_df['bvid'][i]\n",
    "        # 获取视频oid和标题\n",
    "        oid,title = get_information(bv)\n",
    "        # 评论起始页（默认为空）\n",
    "        next_pageID = ''\n",
    "        # 初始化评论数量\n",
    "        count = 0\n",
    "        # --- 以下是实现断点续爬的核心修改 ---\n",
    "        \n",
    "        comment_csv_path = f'./data/{bv}_comment.csv'\n",
    "        progress_json_path = f'./data/{bv}_progress.json'\n",
    "\n",
    "        next_pageID = ''\n",
    "        count = 0\n",
    "        file_open_mode = 'w'\n",
    "        write_header = True\n",
    "        is_second = True\n",
    "\n",
    "        # 检查是否存在进度文件\n",
    "        if os.path.exists(progress_json_path):\n",
    "            try:\n",
    "                with open(progress_json_path, 'r') as f:\n",
    "                    progress_data = json.load(f)\n",
    "                    next_pageID = progress_data.get('next_pageID', '')\n",
    "                    count = progress_data.get('count', 0)\n",
    "                \n",
    "                file_open_mode = 'a' # 如果恢复，使用追加模式\n",
    "                write_header = False  # 不再写入表头\n",
    "                print(f\"[{bv}] 检测到进度文件，将从 {count} 条评论后继续。\")\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"[{bv}] 进度文件读取失败: {e}。将从头开始爬取。\")\n",
    "                # 如果进度文件损坏，则重置为默认值\n",
    "                next_pageID, count, file_open_mode, write_header = '', 0, 'w', True\n",
    "        elif os.path.exists(comment_csv_path):\n",
    "            print(f'删除{bv}，重新爬取')\n",
    "            os.remove(comment_csv_path)\n",
    "            \n",
    "\n",
    "        try:\n",
    "            with open(comment_csv_path, mode=file_open_mode, newline='', encoding='utf-8-sig') as file:\n",
    "                csv_writer = csv.writer(file)\n",
    "                if write_header:\n",
    "                    csv_writer.writerow(['序号', '上级评论ID', '评论ID', '用户ID', '用户名', '用户等级', '性别', '评论内容', '评论时间', '回复数', '点赞数', 'IP属地', '是否是大会员'])\n",
    "\n",
    "                # 使用一个标志位确保循环至少能进入一次\n",
    "                is_first_loop = True\n",
    "                while next_pageID != 0 or is_first_loop:\n",
    "                    is_first_loop = False # 进入后立即置为False\n",
    "                    \n",
    "                    # 执行爬取\n",
    "                    bv, oid, next_pageID, count, csv_writer, is_second = start(bv, oid, next_pageID, count, csv_writer, is_second)\n",
    "\n",
    "                    # 如果还有下一页，保存当前进度\n",
    "                    if next_pageID != 0:\n",
    "                        with open(progress_json_path, 'w') as f:\n",
    "                            json.dump({'next_pageID': next_pageID, 'count': count}, f)\n",
    "            \n",
    "            # 任务成功完成后，删除进度文件\n",
    "            if os.path.exists(progress_json_path):\n",
    "                os.remove(progress_json_path)\n",
    "            \n",
    "            print(f'任务{bv}已完成！')\n",
    "\n",
    "        except Exception as e:\n",
    "            # 如果发生任何错误（如网络中断、键盘中断），程序会在这里退出\n",
    "            # 此时进度文件会保留下来，供下次运行使用\n",
    "            print(f\"任务 {bv} 意外中断: {e}\")\n",
    "            print(f\"当前进度已保存至 {progress_json_path}。下次运行时将自动恢复。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import re\n",
    "import os\n",
    "import emoji\n",
    "import opencc\n",
    "from cleantext import clean\n",
    "import ftfy\n",
    "import math\n",
    "import unicodedata as ucd\n",
    "import regex \n",
    "import emoji\n",
    "import opencc\n",
    "\n",
    "# =========================\n",
    "# 预编译/单例资源\n",
    "# =========================\n",
    "\n",
    "_OPENCC_T2S = opencc.OpenCC(\"t2s\")  # 复用 OpenCC 实例\n",
    "\n",
    "# [xxx_yyy]（至少一个下划线）\n",
    "_PAT_BILI_BRACKET = re.compile(r'\\[([^\\[\\]]*?[_-][^\\[\\]]*?)\\]')\n",
    "\n",
    "# 行首“回复/回覆/回復 … ：/:”前缀\n",
    "_PAT_REPLY_PREFIX = re.compile(r'(?s)^\\s*(?:回复|回覆|回復)\\s*@?.*?[：:]\\s*')\n",
    "\n",
    "# ᐠ( ᑒ )ᐟ → [欢呼]（括号内空格宽松）\n",
    "_PAT_CHEER = re.compile(r\"ᐠ\\(\\s*ᑒ\\s*\\)ᐟ\")\n",
    "\n",
    "# 连续句号（全/半角，允许空白夹杂）→ 单个 “…”\n",
    "_PAT_ELLIPSIS = re.compile(r'[。\\.．·・](?:\\s*[。\\.．·・]){1,}')\n",
    "\n",
    "# Default-Ignorable 与 Unicode 空白（clean 阶段用）\n",
    "_PAT_DEFAULT_IGNORABLE = regex.compile(r'\\p{Default_Ignorable_Code_Point}+')\n",
    "_PAT_ZS = regex.compile(r'\\p{Z}+')\n",
    "\n",
    "# 直删 kaomoji 模板 → 构造“空格宽松”的交替正则\n",
    "zap = re.compile(r'[\\u200B-\\u200D\\u2060\\u00AD\\uFEFF]')\n",
    "with open('emoji_list.txt','r',encoding='utf-8') as f:\n",
    "    emoji_list = [zap.sub('', line).rstrip('\\n') for line in f]\n",
    "\n",
    "\n",
    "_KAOMOJI_RAW = [\n",
    "    \"→_→\",\n",
    "    \"(♡ ὅ ◡ ὅ )ʃ♡\",\n",
    "    \"(/^▽^)/\",\n",
    "    \"(>y<)\",\n",
    "    \"(/TДT)/\",\n",
    "    \"(/∇/)\",\n",
    "    \"(ノ)`ω´(ヾ)\",\n",
    "    \"₍˄·͈༝·͈˄*₎◞ ̑!\",\n",
    "    \"_(:D)∠)_\",\n",
    "    \"(¦3【▓】\",\n",
    "    \"ಥ_ಥ\",\n",
    "    \"(T▽T)\",\n",
    "    \"ᕙ(o‸o)ᕗ\",\n",
    "    \"੧ᐛ੭\",\n",
    "    \"(▼へ▼メ)\",\n",
    "    \"(^O^)y\",\n",
    "    \"=-=\",\n",
    "    \"(∠・ω< )⌒★\"\n",
    "]\n",
    "_KAOMOJI_RAW.extend(emoji_list)\n",
    "_KAOMOJI_RAW= list(set(_KAOMOJI_RAW))\n",
    "def _compile_kaomoji_union(raw_list):\n",
    "    parts = []\n",
    "    for s in raw_list:\n",
    "        p = re.escape(s)\n",
    "        # 模板中的（一个或多个）空格 → \\s*（空格宽松）\n",
    "        p = re.sub(r'\\\\\\s+', r'\\\\s*', p)\n",
    "        parts.append(p)\n",
    "    if not parts:\n",
    "        return None\n",
    "    return re.compile(r'(?:' + '|'.join(parts) + r')')\n",
    "_PAT_KAOMOJI_UNION = _compile_kaomoji_union(_KAOMOJI_RAW)\n",
    "\n",
    "FTZ = (\n",
    "    \"_light_skin_tone\",\n",
    "    \"_medium_light_skin_tone\",\n",
    "    \"_medium_skin_tone\",\n",
    "    \"_medium_dark_skin_tone\",\n",
    "    \"_dark_skin_tone\",\n",
    ")\n",
    "\n",
    "# 严格白名单（首项为 canonical）\n",
    "alias_groups = [\n",
    "    [\"爱心\", \"red_heart\", \"heart\", \"heart_suit\", \"心\"],\n",
    "    [\"心碎\", \"broken_heart\", ],\n",
    "    [\"点赞\", \"thumbs_up\", \"赞\", \"赞了\"],\n",
    "    [\"ok\", \"OK\", \"Ok\", \"OK_button\", \"ok_button\"],\n",
    "    [\"保佑\", \"folded_hands\", \"祈祷\", \"祈愿\"],\n",
    "    [\"鼓掌\", \"clapping_hands\",],\n",
    "    [\"胜利\", \"victory_hand\", \"耶\"],\n",
    "    [\"玫瑰\",\"rose\",],\n",
    "    [\"问号\", \"red_question_mark\", \"?\", \"黑人问号\"],\n",
    "    [\"飞吻\",\"face_blowing_a_kiss\",],\n",
    "    [\"吃瓜\", \"吃瓜ing\"],\n",
    "]\n",
    "\n",
    "# 建立严格映射（键统一为小写）\n",
    "alias_map = {}\n",
    "for group in alias_groups:\n",
    "    canon = group[0].lower()\n",
    "    for name in group:\n",
    "        alias_map[name.lower()] = canon\n",
    "\n",
    "# 对单个方括号内容做规范化\n",
    "def canon_name(name: str) -> str:\n",
    "    x = name.lower()\n",
    "    # 1) 先处理肤色：如果命中，去色得到 base\n",
    "    had_tone = False\n",
    "    for suf in FTZ:\n",
    "        if x.endswith(suf):\n",
    "            x = x[: -len(suf)]\n",
    "            had_tone = True\n",
    "            break\n",
    "    # 2) 严格白名单同义映射\n",
    "    if x in alias_map:\n",
    "        return alias_map[x]\n",
    "    # 3) 仅肤色变体：去色后直接用 base\n",
    "    if had_tone:\n",
    "        return x\n",
    "    # 4) 否则保持原样（不改变大小写/写法）\n",
    "    return name\n",
    "\n",
    "# 将字符串中每个 [...] 替换为规范名\n",
    "_PAT_EMOJI = re.compile(r\"\\[([^\\[\\]]+)\\]\")\n",
    "\n",
    "def merge_emoji(text: str) -> str:\n",
    "    return _PAT_EMOJI.sub(lambda m: f\"[{canon_name(m.group(1))}]\", text)\n",
    "\n",
    "# 括号符号块的“左右标记”，保持原集合（为兼容性预先按长度降序）\n",
    "_LEFT_TOKENS  = sorted([\"(\", \"^\", \"•\", \"|\", \">\", \"\\\\\",\"━\",\"ᗜ\",\"⌓\",\"₎\",\"ノ\",\"ヾ\",\"୧\",\"✪\",\"╯\",\"ʕ\"],\n",
    "                       key=len, reverse=True)\n",
    "_RIGHT_TOKENS = sorted([\")\", \"^\", \"•\", \"|\", \"<\", \"/\",\"━\",\"ᗜ\",\"⌓\",\"₎\",\"ノ\",\"୨\",\"✪\",\"╰\",\"ʔ\",\"】\"],\n",
    "                       key=len, reverse=True)\n",
    "\n",
    "# clean 阶段的翻译与白名单（与现有逻辑保持一致）\n",
    "_ALLOWED_SCRIPTS = (\"Han\", \"Hiragana\", \"Katakana\", \"Hangul\", \"Latin\")\n",
    "_ALLOW_PUNCT = set(r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\") | set(\n",
    "    \"，。！？：；、（）《》〈〉“”‘’【】〖〗「」『』〔〕—…·・\"\n",
    ")\n",
    "_ALLOW_SYMBOL = set(\"°×÷￥¥€…−\")\n",
    "_TRANSLATE_MAP = {\n",
    "    0x2013: ord('-'), 0x2014: ord('-'), 0x2212: ord('-'),\n",
    "    ord('『'): ord('「'), ord('』'): ord('」'),\n",
    "    ord('〖'): ord('【'), ord('〗'): ord('】'),\n",
    "    ord('〈'): ord('《'), ord('〉'): ord('》'),\n",
    "    ord('〔'): ord('('), ord('〕'): ord(')'),\n",
    "    ord('·'): ord('・'),\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 统一的字符分类（保持既有语义）\n",
    "# =========================\n",
    "\n",
    "def _is_wordlike_basic(ch: str) -> bool:\n",
    "    \"\"\"与现有 _is_en/_is_digit/_is_cn_basic/_is_japanese/_is_korean 一致的“文字类”判定。\n",
    "    - 英文仅 ASCII A–Z/a–z\n",
    "    - 数字仅 ASCII 0–9\n",
    "    - 中文仅基本区 U+4E00–U+9FFF\n",
    "    - 日/韩按原有区间\n",
    "    \"\"\"\n",
    "    if 'A' <= ch <= 'Z' or 'a' <= ch <= 'z':\n",
    "        return True\n",
    "    if '0' <= ch <= '9':\n",
    "        return True\n",
    "    if '\\u4e00' <= ch <= '\\u9fff':\n",
    "        return True\n",
    "    if ('\\u3040' <= ch <= '\\u309f') or ('\\u30a0' <= ch <= '\\u30ff') \\\n",
    "       or ('\\u31f0' <= ch <= '\\u31ff') or ('\\uff65' <= ch <= '\\uff9f'):\n",
    "        return True  # Japanese\n",
    "    if ('\\u1100' <= ch <= '\\u11ff') or ('\\u3130' <= ch <= '\\u318f') \\\n",
    "       or ('\\ua960' <= ch <= '\\ua97f') or ('\\ud7b0' <= ch <= '\\ud7ff') \\\n",
    "       or ('\\uac00' <= ch <= '\\ud7af') or ('\\uffa0' <= ch <= '\\uffdc'):\n",
    "        return True  # Korean\n",
    "    return False\n",
    "\n",
    "def _is_symbol(ch: str, *, punct_as_symbol: bool) -> bool:\n",
    "    \"\"\"统一的 symbol 判定入口：\n",
    "    - 空白与方括号 '[]' 永远不算 symbol（**硬编码保护方括号**）\n",
    "    - 其它是否为 symbol：非“文字类”则认为是 symbol；\n",
    "      若 punct_as_symbol=True，则 Unicode 标点也视为 symbol（括号表情检测用）\n",
    "    \"\"\"\n",
    "    if ch.isspace() or ch in '[]':   # 硬编码保护方括号\n",
    "        return False\n",
    "    if _is_wordlike_basic(ch):\n",
    "        return False\n",
    "    # 标点是否计为 symbol：括号检测需要 True，其余场景为 False\n",
    "    if not punct_as_symbol and ucd.category(ch).startswith('P'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# “后者优先”区间去重（包含关系保留新来的）\n",
    "def _later_wins_add(existing, new_span):\n",
    "    s0, s1, frag = new_span\n",
    "    kept = []\n",
    "    for e0, e1, efrag in existing:\n",
    "        if (e0 >= s0 and e1 <= s1) or (s0 >= e0 and s1 <= e1):\n",
    "            continue\n",
    "        kept.append((e0, e1, efrag))\n",
    "    kept.append((s0, s1, frag))\n",
    "    return kept\n",
    "\n",
    "# =========================\n",
    "# 子步骤：括号符号块删除（保持 ratio_thr=1/2）\n",
    "# =========================\n",
    "\n",
    "def _strip_emoticon_with_boundaries(text: str, ratio_thr: float = 1/2):\n",
    "    \"\"\"查找左右标记包裹、内部“括号用符号”占比>ratio_thr 的片段；连同两侧连续符号/空格一起替换为单空格。\"\"\"\n",
    "    def _starts_with_any(s, i, toks):\n",
    "        for t in toks:  # 已按长度降序\n",
    "            if s.startswith(t, i):\n",
    "                return t\n",
    "        return None\n",
    "\n",
    "    spans = []\n",
    "    s = text\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i = 0\n",
    "        n = len(s)\n",
    "        while i < n:\n",
    "            lt = _starts_with_any(s, i, _LEFT_TOKENS)\n",
    "            if not lt:\n",
    "                i += 1; continue\n",
    "            l0, l1 = i, i + len(lt)\n",
    "\n",
    "            # 找第一个右标记\n",
    "            j, rtok = l1, None\n",
    "            while j < n:\n",
    "                rt = _starts_with_any(s, j, _RIGHT_TOKENS)\n",
    "                if rt: rtok = rt; break\n",
    "                j += 1\n",
    "            if not rtok:\n",
    "                i += 1; continue\n",
    "            r0, r1 = j, j + len(rtok)\n",
    "\n",
    "            inner = s[l1:r0]\n",
    "            non_space_len = sum(1 for ch in inner if not ch.isspace())\n",
    "            if non_space_len > 0:\n",
    "                sym_cnt = sum(_is_symbol(ch, punct_as_symbol=True) for ch in inner if not ch.isspace())\n",
    "                if (sym_cnt / non_space_len) > ratio_thr:\n",
    "                    # 向两侧扩展：连续“括号场景的符号”或空格\n",
    "                    L = l0\n",
    "                    while L > 0 and (_is_symbol(s[L-1], punct_as_symbol=True) or s[L-1].isspace()):\n",
    "                        L -= 1\n",
    "                    R = r1\n",
    "                    while R < n and (_is_symbol(s[R], punct_as_symbol=True) or s[R].isspace()):\n",
    "                        R += 1\n",
    "                    frag = s[L:R]\n",
    "                    spans = _later_wins_add(spans, (L, R, frag))\n",
    "                    s = s[:L] + \" \" + s[R:]\n",
    "                    n = len(s); i = L + 1; changed = True\n",
    "                    continue\n",
    "            i = l0 + 1\n",
    "    return s, spans\n",
    "\n",
    "# =========================\n",
    "# 子步骤：统一的 Unicode 清洗（保持现有 clean_unicode_text 行为）\n",
    "# =========================\n",
    "\n",
    "def _script_of(ch: str) -> str | None:\n",
    "    for sc in _ALLOWED_SCRIPTS:\n",
    "        if regex.match(fr'\\p{{Script={sc}}}$', ch):\n",
    "            return sc\n",
    "    return None\n",
    "\n",
    "def _clean_unicode_text(text: str) -> str:\n",
    "    s = ucd.normalize('NFKC', text)\n",
    "    # 1) 删除 Default Ignorable\n",
    "    s = _PAT_DEFAULT_IGNORABLE.sub('', s)\n",
    "    # 2) Unicode 空白折叠\n",
    "    s = _PAT_ZS.sub(' ', s).strip()\n",
    "    # 3) 翻译映射（破折号/书名号等）\n",
    "    s = s.translate(_TRANSLATE_MAP)\n",
    "\n",
    "    out = []\n",
    "    prev_is_space = False\n",
    "    for ch in s:\n",
    "        if ch.isspace():\n",
    "            if not prev_is_space:\n",
    "                out.append(' ')\n",
    "                prev_is_space = True\n",
    "            continue\n",
    "        prev_is_space = False\n",
    "\n",
    "        cat = ucd.category(ch)   # 'Ll','Lo','Nd','Ps','Pe','Po','Sm',...\n",
    "        sc  = _script_of(ch)\n",
    "\n",
    "        # 字母（限定脚本）\n",
    "        if cat[0] == 'L' and sc in _ALLOWED_SCRIPTS:\n",
    "            out.append(ch); continue\n",
    "        # 数字（仅 ASCII）\n",
    "        if cat == 'Nd' and ('0' <= ch <= '9'):\n",
    "            out.append(ch); continue\n",
    "        # 允许的标点\n",
    "        if ch in _ALLOW_PUNCT:\n",
    "            out.append(ch); continue\n",
    "        # 少量符号白名单\n",
    "        if ch in _ALLOW_SYMBOL:\n",
    "            out.append(ch); continue\n",
    "        # 组合附加符号（Mn）默认删除；其他 S*（Symbol）默认删除\n",
    "        if cat == 'Mn' or cat[0] == 'S':\n",
    "            continue\n",
    "        # 其余丢弃\n",
    "    return ''.join(out).strip()\n",
    "\n",
    "# =========================\n",
    "# 子步骤：折叠重复“符号”\n",
    "# =========================\n",
    "\n",
    "def _fold_repeated_symbols(s: str) -> str:\n",
    "    if not s: return s\n",
    "    out, prev, run = [], None, 0\n",
    "    for ch in s:\n",
    "        if prev is None:\n",
    "            prev, run = ch, 1; continue\n",
    "        if ch == prev:\n",
    "            run += 1\n",
    "        else:\n",
    "            if _is_symbol(prev, punct_as_symbol=False) and run >= 2:\n",
    "                out.append(prev)          # 符号连串≥2 → 1\n",
    "            else:\n",
    "                out.append(prev * run)    # 非符号或未重复 → 原样\n",
    "            prev, run = ch, 1\n",
    "    if prev is not None:\n",
    "        if _is_symbol(prev, punct_as_symbol=False) and run >= 2:\n",
    "            out.append(prev)\n",
    "        else:\n",
    "            out.append(prev * run)\n",
    "    return ''.join(out)\n",
    "\n",
    "# =========================\n",
    "# 主入口：单条评论文本清洗（保持现有顺序与效果）\n",
    "# =========================\n",
    "_PAT_UNIT_4PLUS_IGNSPACE_NODIG_TWS = re.compile(r'([^\\s\\d]+?)(?:\\s*\\1){3,}(\\s*)')\n",
    "_PAT_BILI_TUODAN_PREFIX = re.compile(r'\\[(?:脱单)+([^\\[\\]]+?)\\]')\n",
    "_PAT_CIALLO_OUTSIDE = regex.compile(\n",
    "    r'\\[[^\\[\\]]+\\](*SKIP)(*F)|(?<![A-Za-z0-9_])(?:Ciallo|ciallo)(?![A-Za-z0-9_])'\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# —— A) (x)/(√) 翻译（在删除表情之前用）——\n",
    "_CHECK_CHARS = \"√✓✔✅☑🗸\"\n",
    "_CROSS_CHARS = \"xX✗✘✕×❌\"\n",
    "_PAT_PAREN_CROSS_TO_BAD = re.compile(r'\\(\\s*([' + re.escape(_CROSS_CHARS) + r'])\\s*\\)')\n",
    "_PAT_PAREN_CHECK_TO_OK  = re.compile(r'\\(\\s*([' + re.escape(_CHECK_CHARS) + r'])\\s*\\)')\n",
    "_PAT_STANDALONE_CHECK   = re.compile(r'[' + re.escape(_CHECK_CHARS) + r']')\n",
    "\n",
    "# —— C) 时间戳 mm:ss / m:ss —— 仅 00–59 秒，分钟 0–59；左右不粘数字\n",
    "_PAT_TIMESTAMP = re.compile(r'(?<!\\d)(?:[0-5]?\\d)[：:][0-5]\\d(?!\\d)')\n",
    "\n",
    "# —— D) URL 与 BV 号 —— 先 URL、后 BV（独立 token）\n",
    "_PAT_URL = re.compile(r'(?:(?<!\\w)(?:https?|ftp)://|(?<!\\w)www\\.)[^\\s<>\"“”‘’]+')\n",
    "_PAT_BV  = re.compile(r'(?<![A-Za-z0-9])BV[A-Za-z0-9]{10}(?![A-Za-z0-9])')\n",
    "\n",
    "# —— B) 激活码（C……N，刚好长度10，独立 token）——\n",
    "_PAT_ACTIVATION = re.compile(r'(?<![A-Za-z0-9])C[A-Z0-9]{8}N(?![A-Za-z0-9])')\n",
    "\n",
    "def shrink_runs_to_three(s: str) -> str:\n",
    "    if not s:\n",
    "        return s\n",
    "    prev = None\n",
    "    while prev != s:  # 用于处理像 \"aaaab\"*4 -> \"aaab\"*3 的二次收敛\n",
    "        prev = s\n",
    "        s = _PAT_UNIT_4PLUS_IGNSPACE_NODIG_TWS.sub(r'\\1\\1\\1\\2', s)\n",
    "    return s\n",
    "\n",
    "def clean_comment_text_pipeline(text: object) -> str:\n",
    "    \"\"\"\n",
    "    完整文本清洗流水线（单条字符串）——保持与现有代码“可观察结果”一致：\n",
    "      1) 规范化 B站方括号表情：[xxx_yyy] → 若末尾为 _金箍 则去该后缀，否则保留最后一段：[a_b_c]→[c]\n",
    "      2) emoji → [alias]（emoji.demojize；保留方括号本体）\n",
    "      3) OpenCC t2s（繁→简）\n",
    "      4) 初步空白统一：将 \\r \\n \\t → 空格，并折叠空白\n",
    "      5) 删除行首“回复/回覆/回復 … ：/:”前缀\n",
    "      6) 符号与颜文字处理（与现有 preprocess_symbol_squeeze 等价）：\n",
    "         6.1) 直删若干 kaomoji 模板（宽松空格；后者优先去重）\n",
    "         6.2) 再次 NFKC（保持现有两次 NFKC 的顺序，以维持完全一致结果）\n",
    "         6.3) 'ᐠ( ᑒ )ᐟ' → '[欢呼]'         \n",
    "         6.4) 括号符号块（内部符号占比>1/2）连同两侧相连符号/空格 → 单空格\n",
    "         6.5) _clean_unicode_text：再次 NFKC + Default-Ignorable 删除 + 脚本/标点/符号白名单过滤\n",
    "         6.4) 连续句号（全/半角，允许空白夹杂）→ 单个“…”， 再次删除多余空格\n",
    "         6.7) 折叠连续相同“符号”为 1 个\n",
    "    重要：方括号字符 '[' 和 ']' **始终**被硬编码为不参与任何删除/折叠判定（但括号内的文字仍按既有规则改写）。\n",
    "    \"\"\"\n",
    "    # —— 入参标准化（与现有行为一致，NaN/None → \"\"）\n",
    "    if text is None:\n",
    "        s = \"\"\n",
    "    elif isinstance(text, float) and math.isnan(text):\n",
    "        s = \"\"\n",
    "    else:\n",
    "        s = str(text)\n",
    "        \n",
    "    # 1) B站方括号表情裁剪\n",
    "    def _bili_sub(m: re.Match) -> str:\n",
    "        inner = m.group(1)\n",
    "        if inner.endswith(\"_金箍\"):\n",
    "            new_inner = inner[:-len(\"_金箍\")]\n",
    "        else:\n",
    "            new_inner = inner.rsplit(\"_\", 1)[-1] \n",
    "            new_inner = new_inner.rsplit(\"-\", 1)[-1]\n",
    "        low = inner.lower()\n",
    "        if \"问号\" in inner:\n",
    "            return \"[问号]\"\n",
    "        if \"点赞\" in inner:\n",
    "            return \"[点赞]\"\n",
    "        if \"ciallo\" in low:\n",
    "            return \"[Ciallo]\"\n",
    "        return f\"[{new_inner}]\"\n",
    "\n",
    "    s = _PAT_BILI_BRACKET.sub(_bili_sub, s)\n",
    "    s = _PAT_BILI_TUODAN_PREFIX.sub(lambda m: f\"[{m.group(1)}]\", s)\n",
    "    s = _PAT_CIALLO_OUTSIDE.sub('[Ciallo]', s)\n",
    "    s = merge_emoji(s)\n",
    "\n",
    "    # [新增 - A] 先把 (x) 系 和 √ 系翻译成标签（在删除表情之前）\n",
    "    s = _PAT_PAREN_CROSS_TO_BAD.sub(\"[错]\", s)   # (x / × / ✗ / ✘ / ✕ / ❌) → [错]\n",
    "    s = _PAT_PAREN_CHECK_TO_OK.sub(\"[对]\", s)    # (√ / ✓ / ✔ / ✅ / ☑ / 🗸) → [对]\n",
    "    s = _PAT_STANDALONE_CHECK.sub(\"[对]\", s)     # 独立的 √ 系符号 → [对]\n",
    "    # 2) emoji → [alias]\n",
    "    s = emoji.demojize(s, language=\"en\", delimiters=(\"[\", \"]\"))\n",
    "\n",
    "    # 3) 繁→简\n",
    "    s = _OPENCC_T2S.convert(s)\n",
    "\n",
    "    # 4) 初步空白统一（等价于你原先的 cleantext 用途）\n",
    "    s = re.sub(r'[\\r\\n\\t]+', ' ', s)\n",
    "    s = _PAT_ZS.sub(' ', s).strip()\n",
    "\n",
    "    # [新增 - D] URL → [链接]（优先整段 URL）\n",
    "    s = _PAT_URL.sub(\"[链接]\", s)\n",
    "    # [新增 - D] BV 号 → [链接]（独立 token）\n",
    "    s = _PAT_BV.sub(\"[链接]\", s)\n",
    "\n",
    "    # [新增 - C] 时间戳（m:ss / mm:ss）→ [时间戳]\n",
    "    s = _PAT_TIMESTAMP.sub(\"[时间戳]\", s)\n",
    "\n",
    "    # [新增 - B] 激活码（C********N，长度10）→ 删除\n",
    "    s = _PAT_ACTIVATION.sub(\"\", s)\n",
    "\n",
    "    # 5) 删除行首“回复 … ：/:”前缀（只清行首）\n",
    "    s = _PAT_REPLY_PREFIX.sub('', s, count=1)\n",
    "\n",
    "    # 6.1) 直删 kaomoji 模板（宽松空格；后者优先）\n",
    "    if _PAT_KAOMOJI_UNION:\n",
    "        spans = [(m.start(), m.end(), m.group(0)) for m in _PAT_KAOMOJI_UNION.finditer(s)]\n",
    "        kept = []\n",
    "        for sp in spans:\n",
    "            kept = _later_wins_add(kept, sp)\n",
    "        for st, ed, _ in sorted(kept, key=lambda x: x[0], reverse=True):\n",
    "            s = s[:st] + \"\" + s[ed:]\n",
    "\n",
    "    # 6.2) NFKC\n",
    "    s = ucd.normalize('NFKC', s)\n",
    "    s = ftfy.fix_text(s)\n",
    "\n",
    "    # 6.3) 特例映射\n",
    "    s = _PAT_CHEER.sub(\"[欢呼]\", s)\n",
    "\n",
    "\n",
    "\n",
    "    # 6.4) 括号符号块删除（内部符号>1/2） \n",
    "    s, _ = _strip_emoticon_with_boundaries(s, ratio_thr=1/2)\n",
    "\n",
    "    # 6.5) 统一 Unicode 清洗（NFKC + DefaultIgnorable + 白名单过滤）\n",
    "    s = _clean_unicode_text(s)\n",
    "\n",
    "    # 6.6) 连续句号 → “…” 再次删除多余空格\n",
    "    s = _PAT_ELLIPSIS.sub(\"…\", s)\n",
    "    s = _PAT_ZS.sub(' ', s).strip()\n",
    "    \n",
    "    \n",
    "    # 6.7) 折叠重复“符号”（[]/空白/文字类不参与）\n",
    "    s = _fold_repeated_symbols(s)\n",
    "    s = shrink_runs_to_three(s)\n",
    "\n",
    "    # _PAT_BRACKET_TOKEN = re.compile(r'\\[[^\\[\\]]+?\\]')\n",
    "    # for tok in _PAT_BRACKET_TOKEN.findall(s):\n",
    "    #     emote_counter[tok] = emote_counter.get(tok, 0) + 1\n",
    "    return s\n",
    "\n",
    "# 列级便捷封装（可选）\n",
    "def clean_comment_series(series):\n",
    "    \"\"\"对 pandas Series 应用文本清洗。调用方负责确保 dtype 为 str 或可被 str() 化。\"\"\"\n",
    "    return series.map(clean_comment_text_pipeline)\n",
    "\n",
    "def resolve_duplicate_comment_ids(df: pd.DataFrame,\n",
    "                                  id_col: str = \"评论ID\",\n",
    "                                  content_col: str = \"评论内容\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    检查相同 评论ID 是否对应多个 row。\n",
    "    情况1：同一 评论ID 的 评论内容 全部一致(经过清洗后的一致)：\n",
    "        - 打印一条说明\n",
    "        - 只保留一行，其他行删除\n",
    "    情况2：同一 评论ID 的 评论内容 存在不一致：\n",
    "        - 打印⚠警告并列出该ID下的所有不同版本内容\n",
    "        - 暂不自动删这些行（保留原始多版本，后续人工判断）\n",
    "\n",
    "    返回: 处理后的 df（已去除 情况1 的纯重复行）。\n",
    "    约定：不做防御性编程，假设列存在且为字符串 dtype。\n",
    "    \"\"\"\n",
    "    # 我们先复制，避免原 df 原地修改导致调用方困惑\n",
    "    out = df.copy()\n",
    "\n",
    "    # 先按 评论ID 分组，聚合每组的唯一 评论内容\n",
    "    grp = out.groupby(id_col)[content_col].agg(lambda x: list(pd.unique(x)))\n",
    "\n",
    "    # 找到两类ID\n",
    "    ids_all_same = grp[grp.map(lambda uniq_list: len(uniq_list) == 1)]\n",
    "    ids_conflict = grp[grp.map(lambda uniq_list: len(uniq_list) > 1)]\n",
    "\n",
    "    # --- 情况2：冲突 (同ID多内容) -> 打印警告\n",
    "    if not ids_conflict.empty:\n",
    "        print(\"⚠ 以下 评论ID 映射到多个不同的评论内容，需人工检查：\")\n",
    "        for cid, contents in ids_conflict.items():\n",
    "            print(f\"评论ID={cid}\")\n",
    "            for idx_c, c in enumerate(contents, start=1):\n",
    "                print(f\"    版本{idx_c}: {c}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    # --- 情况1：完全相同内容的重复 -> 我们只保留一条\n",
    "    # 思路：对这些ID对应的行，基于 [评论ID, 评论内容] 做drop_duplicates，其他列随第一条\n",
    "    if not ids_all_same.empty:\n",
    "        # mask1: 属于这批“安全可折叠”的ID\n",
    "        mask_same = out[id_col].isin(ids_all_same.index)\n",
    "        same_chunk = out.loc[mask_same]\n",
    "\n",
    "        # 在 same_chunk 内部，按 (评论ID, 评论内容) 去重\n",
    "        dedup_same_chunk = same_chunk.drop_duplicates(subset=id_col, keep=\"first\")\n",
    "\n",
    "        # mask2: 其他行（未触发情况1，或者是冲突ID等）\n",
    "        mask_other = ~mask_same\n",
    "        other_chunk = out.loc[mask_other]\n",
    "\n",
    "        # 合并回来，保持原index顺序的相对稳定性：先排序索引再concat\n",
    "        out = (pd.concat([dedup_same_chunk, other_chunk], axis=0)\n",
    "                 .sort_index(kind=\"stable\"))\n",
    "\n",
    "        # 打个summary方便我们知道删了多少\n",
    "        removed_rows = len(same_chunk) - len(dedup_same_chunk)\n",
    "        print(f\"已合并相同 评论ID & 相同 评论内容 的重复行，共移除 {removed_rows} 行。\")\n",
    "    else:\n",
    "        print(\"未发现可以安全折叠的(评论ID相同 & 评论内容一致)重复行。\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def _ensure_cols_exist(df, cols):\n",
    "    miss = set(cols) - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV缺少必要列: {miss}\")\n",
    "\n",
    "def find_dup_by_user_and_content(df, min_count: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"同一 用户ID + 评论内容 视作完全一致；返回包含重复组的行，并打印重复次数>=min_count的组。\"\"\"\n",
    "    key = [\"用户ID\", \"评论内容\"]\n",
    "    _ensure_cols_exist(df, key)\n",
    "    dup_mask = df.duplicated(subset=key, keep=False)\n",
    "    dup = df.loc[dup_mask].copy()\n",
    "    if dup.empty:\n",
    "        print(\"无完全一致的重复评论。\")\n",
    "        return dup\n",
    "    dup[\"重复组大小\"] = dup.groupby(key)[\"评论ID\" if \"评论ID\" in df.columns else \"评论内容\"].transform(\"size\")\n",
    "    dup5 = dup[dup[\"重复组大小\"] >= min_count].copy()\n",
    "    if dup5.empty:\n",
    "        print(f\"无重复次数≥{min_count}的完全一致评论。\")\n",
    "    else:\n",
    "        # 代表行：按组大小降序、点赞数（若存在）降序、用户ID升序，取每组第一条\n",
    "        dup5[\"_like\"] = pd.to_numeric(dup5[\"点赞数\"], errors=\"coerce\").fillna(0)\n",
    "        rep = (dup5.sort_values([\"重复组大小\",\"_like\",\"用户ID\"], ascending=[False, False, True])\n",
    "                    .drop_duplicates(subset=key, keep=\"first\"))\n",
    "        rep = rep.drop(columns=[\"_like\"])\n",
    "\n",
    "        cols_to_show = [c for c in [\"用户ID\",\"评论内容\",\"评论ID\",\"点赞数\",\"评论时间\",\"重复组大小\"] if c in rep.columns]\n",
    "        print(rep.loc[:, cols_to_show].to_string(index=False))\n",
    "    return dup\n",
    "\n",
    "def dedupe_keep_max_like(df) -> pd.DataFrame:\n",
    "    \"\"\"对完全一致评论（用户ID+评论内容）去重，只保留点赞数最多的一条。\"\"\"\n",
    "    _ensure_cols_exist(df, [\"用户ID\", \"评论内容\", \"点赞数\"])\n",
    "    tmp = df.copy()\n",
    "    tmp[\"_like\"] = pd.to_numeric(tmp[\"点赞数\"], errors=\"coerce\").fillna(0)\n",
    "    # 每组取点赞数最大的索引；若并列，取首个出现\n",
    "    idx = tmp.groupby([\"用户ID\",\"评论内容\"])[\"_like\"].idxmax()\n",
    "    dedup = tmp.loc[idx].drop(columns=[\"_like\"]).sort_index()\n",
    "    return dedup\n",
    "\n",
    "def print_frequent_contents_after_clean(dedup_df, min_count: int = 5):\n",
    "    \"\"\"在去重后的数据上，仅按评论内容统计，打印出现次数≥min_count的评论内容及次数。\"\"\"\n",
    "    _ensure_cols_exist(dedup_df, [\"评论内容\"])\n",
    "    counts = dedup_df[\"评论内容\"].value_counts()\n",
    "    hot = counts[counts >= min_count].rename_axis(\"评论内容\").reset_index(name=\"出现次数\")\n",
    "    if hot.empty:\n",
    "        print(f\"清洗后无出现次数≥{min_count}的评论内容。\")\n",
    "    else:\n",
    "        display(hot.style.set_properties(subset=[\"评论内容\"]))\n",
    "        # print(hot)\n",
    "    return hot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CSV_FILENAME = \"./data/bilibili_video_data_final.csv\"\n",
    "CHARA_FILENAME='./data/character_info.csv'\n",
    "video_df = pd.read_csv(CSV_FILENAME)\n",
    "check_video_index_list=video_df.index\n",
    "all_nonstd_symbols = set()\n",
    "bad_idx=[]\n",
    "for i in check_video_index_list:\n",
    "    video_bvid = video_df['bvid'][i]  # example path provided by you\n",
    "    comment_filename = f'./data/{video_bvid}_comment.csv'\n",
    "    preprocessed_filename = f'./data/{video_bvid}_comment_preprocessed.csv'\n",
    "    # Read\n",
    "    if os.path.exists(comment_filename):\n",
    "        # print(f'{video_bvid} 存在')\n",
    "        df = pd.read_csv(comment_filename, encoding=\"utf-8-sig\", dtype=str)\n",
    "        df = resolve_duplicate_comment_ids(df, id_col=\"评论ID\", content_col=\"评论内容\")\n",
    "        rate = len(df)/video_df['comment_count'][i]\n",
    "        if rate<0.95:\n",
    "            print(f'记录长度{video_df['comment_count'][i]}， 处理后长度{len(df)}，比值为{rate}')\n",
    "            bad_idx.append(i)\n",
    "    else:\n",
    "        print(f'{video_bvid} 不存在')\n",
    "\n",
    "    df['评论内容'] = clean_comment_series(df['评论内容'])\n",
    "\n",
    "\n",
    "    # ==== 调用 ====\n",
    "    # 1) 找到完全一致的重复，并打印重复次数≥5的评论\n",
    "    # dup_df = find_dup_by_user_and_content(df, min_count=5)\n",
    "    # print(len(dup_df))\n",
    "\n",
    "    # 2) 基于全部评论去重：对(用户ID, 评论内容)相同的，只保留点赞数最大的    \n",
    "    df = df[df[\"评论内容\"].fillna(\"\").str.strip().ne(\"\")].copy()\n",
    "    dedup_df = dedupe_keep_max_like(df)\n",
    "\n",
    "\n",
    "    # 3) 在去重后的数据上，打印出现次数≥5的评论内容（不要求用户ID相同）\n",
    "    # hot = print_frequent_contents_after_clean(dedup_df, min_count=5)\n",
    "\n",
    "    dedup_df.to_csv(preprocessed_filename, index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83906ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal, fail-fast Python to compute time-to-banner (positive before, negative after)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Inputs (assumed to exist) ---\n",
    "VIDEO_FILENAME = \"bilibili_video_data_final.csv\"\n",
    "CHARA_FILENAME = 'character_info.csv'\n",
    "# --- Load ---\n",
    "chars = pd.read_csv(CHARA_FILENAME)\n",
    "videos = pd.read_csv(VIDEO_FILENAME)\n",
    "\n",
    "# --- Parse dates ---\n",
    "# Start date has canonical time 04:00 (local-naive); enforce by normalizing date and adding 4h.\n",
    "chars[\"start_date\"] = pd.to_datetime(chars[\"start_date\"], errors=\"raise\")\n",
    "chars[\"start_datetime\"] = chars[\"start_date\"].dt.normalize() + pd.Timedelta(hours=4)\n",
    "\n",
    "# Publish date may already include time; if it's date-only it becomes 00:00 same day.\n",
    "videos[\"publish_datetime\"] = pd.to_datetime(videos[\"publish_date\"], errors=\"raise\")\n",
    "\n",
    "# --- Merge (many videos to one character) ---\n",
    "df = videos.merge(\n",
    "    chars[[\"character\", \"start_datetime\"]],\n",
    "    on=\"character\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ")\n",
    "\n",
    "# --- Compute distance: positive if before banner, negative if after ---\n",
    "delta = df[\"start_datetime\"] - df[\"publish_datetime\"]\n",
    "df[\"time_to_banner_hours\"] = delta.dt.total_seconds() / 3600.0  # optional: hours\n",
    "\n",
    "# --- Save and preview ---\n",
    "OUT_PATH = \"./data/bilibili_video_with_time_to_banner.csv\"\n",
    "df.to_csv(OUT_PATH, index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab74e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) 载入视频元数据\n",
    "videos = pd.read_csv(\n",
    "    \"./data/bilibili_video_with_time_to_banner.csv\",\n",
    "    parse_dates=[\"publish_datetime\"]\n",
    ")\n",
    "\n",
    "# 2) 逐视频读取评论、增列并收集\n",
    "dfs = []\n",
    "for _, row in videos[[\"bvid\", \"character\", \"video_type\", \"publish_datetime\"]].iterrows():\n",
    "    bvid = row[\"bvid\"]\n",
    "    pub_time = row[\"publish_datetime\"]\n",
    "\n",
    "    cmt = pd.read_csv(\n",
    "        f\"./data/{bvid}_comment_preprocessed.csv\",\n",
    "        parse_dates=[\"评论时间\"]\n",
    "    )\n",
    "\n",
    "    # 只保留必要列\n",
    "    cmt = cmt[[\n",
    "        \"上级评论ID\", \"评论ID\", \"用户ID\",\n",
    "        \"评论内容\", \"评论时间\",\n",
    "        \"回复数\", \"点赞数\"\n",
    "    ]]\n",
    "\n",
    "    # 增加三列：character、video_type、是否在24小时内\n",
    "    cmt[\"character\"] = row[\"character\"]\n",
    "    cmt[\"video_type\"] = row[\"video_type\"]\n",
    "    cmt[\"within_24h\"] = (\n",
    "        (cmt[\"评论时间\"] >= pub_time) &\n",
    "        (cmt[\"评论时间\"] <  pub_time + pd.Timedelta(hours=24))\n",
    "    ).astype(int)\n",
    "\n",
    "    dfs.append(cmt)\n",
    "\n",
    "# 3) 合并所有评论并输出\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 可选：落盘（只包含重要列 + 新增列）\n",
    "merged.to_csv(\"./data/all_comments_merged.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
